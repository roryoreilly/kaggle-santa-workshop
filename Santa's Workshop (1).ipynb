{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "# from numba import njit, jitclass\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import os\n",
    "import random\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, Flatten, Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "family_data_path = './input/family_data.csv'\n",
    "submission_path = './input/submission-76000.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cost:\n",
    "    def __init__(self, family_data_path = family_data_path):\n",
    "        family = pd.read_csv(family_data_path, index_col='family_id')\n",
    "        self.family_size = family.n_people.values.astype(np.int8)\n",
    "        self.family_cost_matrix = self._penality_array(family, self.family_size)\n",
    "        self.accounting_cost_matrix = self._accounting_cost_matrix(family)\n",
    "        \n",
    "    def _penality_array(self, family, family_size):\n",
    "        penalties = np.asarray([\n",
    "            [\n",
    "                0,\n",
    "                50,\n",
    "                50 + 9 * n,\n",
    "                100 + 9 * n,\n",
    "                200 + 9 * n,\n",
    "                200 + 18 * n,\n",
    "                300 + 18 * n,\n",
    "                300 + 36 * n,\n",
    "                400 + 36 * n,\n",
    "                500 + 36 * n + 199 * n,\n",
    "                500 + 36 * n + 398 * n\n",
    "            ] for n in range(family_size.max() + 1)\n",
    "        ])\n",
    "        family_cost_matrix = np.concatenate(family.n_people.apply(\n",
    "                lambda n: np.repeat(penalties[n, 10], 100).reshape(1, 100)))\n",
    "        for fam in family.index:\n",
    "            for choice_order, day in enumerate(family.loc[fam].drop(\"n_people\")):\n",
    "                family_cost_matrix[fam, day - 1] = penalties[family.loc[fam, \"n_people\"], choice_order]\n",
    "        return family_cost_matrix\n",
    "        \n",
    "    \n",
    "    def _accounting_cost_matrix(self, family):\n",
    "        accounting_cost_matrix = np.zeros((500, 500))\n",
    "        for n in range(accounting_cost_matrix.shape[0]):\n",
    "            for diff in range(accounting_cost_matrix.shape[1]):\n",
    "                accounting_cost_matrix[n, diff] = max(0, (n - 125.0) / 400.0 * n**(0.5 + diff / 50.0))\n",
    "        return accounting_cost_matrix\n",
    "    \n",
    "    def calculate(self, prediction):\n",
    "        p, ac, nl, nh = self._calculate(prediction, self.family_size, self.family_cost_matrix, self.accounting_cost_matrix)\n",
    "        return (p + ac) + (nl + nh) * 1000000\n",
    "        \n",
    "    @staticmethod\n",
    "#     @njit(fastmath=True)\n",
    "    def _calculate(prediction, family_size, family_cost_matrix, accounting_cost_matrix):\n",
    "        N_DAYS = 100\n",
    "        MAX_OCCUPANCY = 300\n",
    "        MIN_OCCUPANCY = 125\n",
    "        penalty = 0\n",
    "        daily_occupancy = np.zeros(N_DAYS + 1, dtype=np.int16)\n",
    "        for i, (pred, n) in enumerate(zip(prediction, family_size)):\n",
    "            daily_occupancy[pred - 1] += n\n",
    "            penalty += family_cost_matrix[i, pred - 1]\n",
    "\n",
    "        accounting_cost = 0\n",
    "        n_low = 0\n",
    "        n_high = 0\n",
    "        daily_occupancy[-1] = daily_occupancy[-2]\n",
    "        for day in range(N_DAYS):\n",
    "            n_next = daily_occupancy[day + 1]\n",
    "            n = daily_occupancy[day]\n",
    "            n_high += (n > MAX_OCCUPANCY) \n",
    "            n_low += (n < MIN_OCCUPANCY)\n",
    "            diff = abs(n - n_next)\n",
    "            accounting_cost += accounting_cost_matrix[n, diff]\n",
    "\n",
    "        return np.asarray([penalty, accounting_cost, n_low, n_high])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Workshop:\n",
    "    def __init__(self):\n",
    "        self.family_sizes = []\n",
    "        self.family_choices = []\n",
    "        self.assigned_days = []\n",
    "        self.changes_left = 0\n",
    "        self.cost = Cost()\n",
    "            \n",
    "    def reset(self):\n",
    "        self._set_state()\n",
    "        self.changes_left = 10000\n",
    "        return self._get_env_state()\n",
    "    \n",
    "    def step(self, family_index, day_choice_index):\n",
    "        self.changes_left -= 1\n",
    "        reward = -self.cost.calculate(self.assigned_days)\n",
    "        self.assigned_days[family_index] = self.family_choices[family_index, day_choice_index]\n",
    "        return self._get_env_state(), reward, self._is_done()\n",
    "    \n",
    "    def get_submission(self):\n",
    "        submission = pd.Series(self.assigned_days, name=\"assigned_day\")\n",
    "        submission.index.name = \"family_id\"\n",
    "        score = self.cost.calculate(self.assigned_days)\n",
    "        return submission, score\n",
    "        \n",
    "    def _set_state(self):\n",
    "        family = pd.read_csv(family_data_path, index_col='family_id')\n",
    "        choice_cols = ['choice_{}'.format(i) for i in range(10)]\n",
    "        self.family_choices = np.array(family[choice_cols])\n",
    "        self.family_sizes = np.array(family['n_people'])\n",
    "        \n",
    "        submission = pd.read_csv(submission_path, index_col='family_id')\n",
    "        self.assigned_days = submission['assigned_day'].values  \n",
    "        \n",
    "    def _get_env_state(self):\n",
    "        return [self.assigned_days, self.family_choices, self.family_sizes]\n",
    "    \n",
    "    def _is_done(self):\n",
    "        return self.changes_left < 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.99\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "    \"\"\"Huber loss for Q Learning\n",
    "    References: https://en.wikipedia.org/wiki/Huber_loss\n",
    "                https://www.tensorflow.org/api_docs/python/tf/losses/huber_loss\n",
    "    \"\"\"\n",
    "\n",
    "    def _huber_loss(self, y_true, y_pred, clip_delta=1.0):\n",
    "        error = y_true - y_pred\n",
    "        cond  = tf.abs(error) <= clip_delta\n",
    "\n",
    "        squared_loss = 0.5 * tf.square(error)\n",
    "        quadratic_loss = 0.5 * tf.square(clip_delta) + clip_delta * (tf.abs(error) - clip_delta)\n",
    "\n",
    "        return tf.mean(tf.where(cond, squared_loss, quadratic_loss))\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        assigned_days = Input(shape=(1,))\n",
    "        family_sizes = Input(shape=(1,))\n",
    "        family_choices = Input(shape=(10,))\n",
    "        \n",
    "        a = Dense(512, activation='relu')(assigned_days)\n",
    "        b = Dense(512, activation='relu')(family_sizes)\n",
    "        c = Flatten()(family_choices)\n",
    "        c = Dense(512, activation='relu')(c)\n",
    "        \n",
    "        x = Concatenate()([a, b, c])\n",
    "        x = Dense(1024, activation='relu')(x)\n",
    "        x = Dense(1024, activation='relu')(x)\n",
    "        x = Dense(50000, activation='softmax')(x)\n",
    "        x = Reshape((5000, 10))(x)\n",
    "        \n",
    "        model = Model([assigned_days, family_choices, family_sizes], [x])\n",
    "        \n",
    "        model.compile(loss=tf.losses.mean_squared_error,\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        # copy weights from model to target_model\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return [random.randint(0, 4999), random.randint(0, 9)]\n",
    "        act_values = self.model.predict(state)\n",
    "        act_values = np.array(act_values)\n",
    "        return np.unravel_index(act_values.argmax(), act_values.shape)  # returns action\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            action_family = action[0]\n",
    "            action_day = action[1]\n",
    "            target = self.model.predict(state)\n",
    "            if done:\n",
    "                target[action_family][action_day] = reward\n",
    "            else:\n",
    "                # a = self.model.predict(next_state)[0]\n",
    "                t = self.target_model.predict(next_state)[0]\n",
    "                target[action_family][action_day] = reward + self.gamma * np.amax(t)\n",
    "                # target[0][action] = reward + self.gamma * t[np.argmax(a)]\n",
    "            self.model.fit(state, target, epochs=1, verbose=1)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/10, changes left: 9999, score: -76177.27504576276, e: 1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-3f93d19af197>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m                   .format(e, episodes, env.changes_left, reward, agent.epsilon))\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_target_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./reinforcement-workshop.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-a9d58543e378>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0maction_family\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0maction_day\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction_family\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction_day\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1491\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1492\u001b[0m       return training_arrays.predict_loop(\n\u001b[0;32m-> 1493\u001b[0;31m           self, x, batch_size=batch_size, verbose=verbose, steps=steps)\n\u001b[0m\u001b[1;32m   1494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1495\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, inputs, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2912\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2914\u001b[0;31m     \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2915\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2916\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "episodes = 10\n",
    "env = Workshop()\n",
    "agent = DQNAgent()\n",
    "# agent.load(\"./reinforcement-workshop.h5\")\n",
    "done = False\n",
    "batch_size = 32\n",
    "\n",
    "for e in range(episodes):\n",
    "    state = env.reset()\n",
    "    while not done:\n",
    "        # env.render()\n",
    "        family_id, day = agent.act(state)\n",
    "        next_state, reward, done = env.step(family_id, day)\n",
    "#         reward = reward if not done else -10\n",
    "#         next_state = np.reshape(next_state, [1, state_size])\n",
    "        agent.remember(state, [family_id, day], reward, next_state, done)\n",
    "        state = next_state\n",
    "        if env.changes_left % 100 == 99:\n",
    "            print(\"episode: {}/{}, changes left: {}, score: {}, e: {:.2}\"\n",
    "                  .format(e, episodes, env.changes_left, reward, agent.epsilon))\n",
    "        if len(agent.memory) % batch_size == 0:\n",
    "            agent.replay(batch_size)\n",
    "    agent.update_target_model()\n",
    "    agent.save(\"./reinforcement-workshop.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "family_size = np.array(list(family_size_dict.values()))\n",
    "days_array = np.array(days)\n",
    "\n",
    "penalties_array = \n",
    "\n",
    "choice_dict_num = [{vv:i for i, vv in enumerate(di.values())} for di in choice_dict.values()]\n",
    "largest_choice_key = max(max(x.keys()) for x in choice_dict_num)\n",
    "choice_array_num = np.array([\n",
    "    [\n",
    "        choice[n] if n in choice else - 1 \n",
    "        for n in range(largest_choice_key+1)\n",
    "    ] \n",
    "    for choice in choice_dict_num\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = submission['assigned_day'].values\n",
    "start_score = cost_function(best, penalties_array, family_size, days_array)\n",
    "\n",
    "# loop over each family\n",
    "for fam_id in tqdm(range(len(best))):\n",
    "    # loop over each family choice\n",
    "    for pick in range(10):\n",
    "        day = choice_dict[fam_id][f'choice_{pick}']\n",
    "        temp = new.copy()\n",
    "        temp[fam_id] = day # add in the new pick\n",
    "        if cost_function(temp, penalties_array, family_size, days_array) < start_score:\n",
    "            new = temp.copy()\n",
    "            start_score = cost_function(new, penalties_array, family_size, days_array)\n",
    "\n",
    "submission['assigned_day'] = new\n",
    "score = cost_function(new, penalties_array, family_size, days_array)\n",
    "submission.to_csv(f'submission_{score}.csv')\n",
    "print(f'Score: {score}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
