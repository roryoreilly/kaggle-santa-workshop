{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "# from numba import njit, jitclass\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import os\n",
    "import random\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "family_data_path = './input/family_data.csv'\n",
    "submission_path = './input/sample_submission.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cost:\n",
    "    def __init__(self, family_data_path = family_data_path):\n",
    "        family = pd.read_csv(family_data_path, index_col='family_id')\n",
    "        self.family_size = family.n_people.values.astype(np.int8)\n",
    "        self.family_cost_matrix = self._penality_array(family, self.family_size)\n",
    "        self.accounting_cost_matrix = self._accounting_cost_matrix(family)\n",
    "        \n",
    "    def _penality_array(self, family, family_size):\n",
    "        penalties = np.asarray([\n",
    "            [\n",
    "                0,\n",
    "                50,\n",
    "                50 + 9 * n,\n",
    "                100 + 9 * n,\n",
    "                200 + 9 * n,\n",
    "                200 + 18 * n,\n",
    "                300 + 18 * n,\n",
    "                300 + 36 * n,\n",
    "                400 + 36 * n,\n",
    "                500 + 36 * n + 199 * n,\n",
    "                500 + 36 * n + 398 * n\n",
    "            ] for n in range(family_size.max() + 1)\n",
    "        ])\n",
    "        family_cost_matrix = np.concatenate(family.n_people.apply(\n",
    "                lambda n: np.repeat(penalties[n, 10], 100).reshape(1, 100)))\n",
    "        for fam in family.index:\n",
    "            for choice_order, day in enumerate(family.loc[fam].drop(\"n_people\")):\n",
    "                family_cost_matrix[fam, day - 1] = penalties[family.loc[fam, \"n_people\"], choice_order]\n",
    "        return family_cost_matrix\n",
    "        \n",
    "    \n",
    "    def _accounting_cost_matrix(self, family):\n",
    "        accounting_cost_matrix = np.zeros((500, 500))\n",
    "        for n in range(accounting_cost_matrix.shape[0]):\n",
    "            for diff in range(accounting_cost_matrix.shape[1]):\n",
    "                accounting_cost_matrix[n, diff] = max(0, (n - 125.0) / 400.0 * n**(0.5 + diff / 50.0))\n",
    "        return accounting_cost_matrix\n",
    "    \n",
    "    def calculate(self, prediction):\n",
    "        p, ac, nl, nh = self._calculate(prediction, self.family_size, self.family_cost_matrix, self.accounting_cost_matrix)\n",
    "        return (p + ac) + (nl + nh) * 1000000\n",
    "        \n",
    "    @staticmethod\n",
    "#     @njit(fastmath=True)\n",
    "    def _calculate(prediction, family_size, family_cost_matrix, accounting_cost_matrix):\n",
    "        N_DAYS = 100\n",
    "        MAX_OCCUPANCY = 300\n",
    "        MIN_OCCUPANCY = 125\n",
    "        penalty = 0\n",
    "        daily_occupancy = np.zeros(N_DAYS + 1, dtype=np.int16)\n",
    "        for i, (pred, n) in enumerate(zip(prediction, family_size)):\n",
    "            daily_occupancy[pred - 1] += n\n",
    "            penalty += family_cost_matrix[i, pred - 1]\n",
    "\n",
    "        accounting_cost = 0\n",
    "        n_low = 0\n",
    "        n_high = 0\n",
    "        daily_occupancy[-1] = daily_occupancy[-2]\n",
    "        for day in range(N_DAYS):\n",
    "            n_next = daily_occupancy[day + 1]\n",
    "            n = daily_occupancy[day]\n",
    "            n_high += (n > MAX_OCCUPANCY) \n",
    "            n_low += (n < MIN_OCCUPANCY)\n",
    "            diff = abs(n - n_next)\n",
    "            accounting_cost += accounting_cost_matrix[n, diff]\n",
    "\n",
    "        return np.asarray([penalty, accounting_cost, n_low, n_high])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Workshop:\n",
    "    def __init__(self):\n",
    "        self.family_sizes = []\n",
    "        self.family_choices = []\n",
    "        self.assigned_days = []\n",
    "        self.changes_left = 0\n",
    "        self.cost = Cost()\n",
    "            \n",
    "    def reset(self):\n",
    "        self._set_state()\n",
    "        self.changes_left = 10000\n",
    "        return self._get_env_state()\n",
    "    \n",
    "    def step(self, family_index, day_choice_index):\n",
    "        self.changes_left -= 1\n",
    "        reward = -self.cost.calculate(self.assigned_days)\n",
    "        self.assigned_days[family_index] = self.family_choices[family_index, day_choice_index]\n",
    "        return self._get_env_state(), reward, self._is_done()\n",
    "    \n",
    "    def get_submission(self):\n",
    "        submission = pd.Series(self.assigned_days, name=\"assigned_day\")\n",
    "        submission.index.name = \"family_id\"\n",
    "        score = self.cost.calculate(self.assigned_days)\n",
    "        return submission, score\n",
    "        \n",
    "    def _set_state(self):\n",
    "        family = pd.read_csv(family_data_path, index_col='family_id')\n",
    "        choice_cols = ['choice_{}'.format(i) for i in range(10)]\n",
    "        self.family_choices = np.array(family[choice_cols])\n",
    "        self.family_sizes = np.array(family['n_people'])\n",
    "        \n",
    "        submission = pd.read_csv(submission_path, index_col='family_id')\n",
    "        self.assigned_days = submission['assigned_day'].values  \n",
    "        \n",
    "    def _get_env_state(self):\n",
    "        return [self.assigned_days, self.family_choices, self.family_sizes]\n",
    "    \n",
    "    def _is_done(self):\n",
    "        return self.changes_left < 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.99\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "    \"\"\"Huber loss for Q Learning\n",
    "    References: https://en.wikipedia.org/wiki/Huber_loss\n",
    "                https://www.tensorflow.org/api_docs/python/tf/losses/huber_loss\n",
    "    \"\"\"\n",
    "\n",
    "    def _huber_loss(self, y_true, y_pred, clip_delta=1.0):\n",
    "        error = y_true - y_pred\n",
    "        cond  = tf.abs(error) <= clip_delta\n",
    "\n",
    "        squared_loss = 0.5 * tf.square(error)\n",
    "        quadratic_loss = 0.5 * tf.square(clip_delta) + clip_delta * (tf.abs(error) - clip_delta)\n",
    "\n",
    "        return tf.mean(tf.where(cond, squared_loss, quadratic_loss))\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        assigned_days = Input(shape=(5000,))\n",
    "        family_sizes = Input(shape=(5000,))\n",
    "        family_choices = Input(shape=(5000,10))\n",
    "        \n",
    "        a = Dense(512, activation='relu')(assigned_days)\n",
    "        b = Dense(512, activation='relu')(family_sizes)\n",
    "        c = Flatten()(family_choices)\n",
    "        c = Dense(512, activation='relu')(c)\n",
    "        \n",
    "        x = Concatenate()([a, b, c])\n",
    "        x = Dense(1024, activation='relu')(x)\n",
    "        x = Dense(1024, activation='relu')(x)\n",
    "        family = Dense(100, activation='softmax')(x)\n",
    "        day = Dense(10, activation='softmax')(x)\n",
    "        \n",
    "        model = Model([assigned_days, family_choices, family_sizes], [family, day])\n",
    "        \n",
    "        model.compile(loss=tf.keras.losses.Huber(),\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        # copy weights from model to target_model\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        print(state)\n",
    "#         if np.random.rand() <= self.epsilon:\n",
    "#             return [np.random.uniform(0.0, 1.0, 100).tolist(), np.random.uniform(0.0, 1.0, 10).tolist()]\n",
    "        act_values = self.model.predict(inputs=state)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = self.model.predict(state)\n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                # a = self.model.predict(next_state)[0]\n",
    "                t = self.target_model.predict(next_state)[0]\n",
    "                target[0][action] = reward + self.gamma * np.amax(t)\n",
    "                # target[0][action] = reward + self.gamma * t[np.argmax(a)]\n",
    "            self.model.fit(state, target, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.keras.losses' has no attribute 'Huber'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-b99834e40600>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mepisodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWorkshop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQNAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# agent.load(\"./reinforcement-workshop.h5\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-9521fd5ea2a3>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.99\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_target_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-9521fd5ea2a3>\u001b[0m in \u001b[0;36m_build_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0massigned_days\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily_choices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily_sizes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfamily\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mday\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         model.compile(loss=tf.keras.losses.Huber(),\n\u001b[0m\u001b[1;32m     47\u001b[0m                       optimizer=Adam(lr=self.learning_rate))\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.keras.losses' has no attribute 'Huber'"
     ]
    }
   ],
   "source": [
    "episodes = 500\n",
    "env = Workshop()\n",
    "agent = DQNAgent()\n",
    "# agent.load(\"./reinforcement-workshop.h5\")\n",
    "done = False\n",
    "batch_size = 8\n",
    "\n",
    "for e in range(episodes):\n",
    "    state = env.reset()\n",
    "    for time in range(500):\n",
    "        # env.render()\n",
    "        action = agent.act(state)\n",
    "        family = np.argmax(action[0])\n",
    "        day = np.argmax(action[1])\n",
    "        next_state, reward, done = env.step(family, day)\n",
    "#         reward = reward if not done else -10\n",
    "#         next_state = np.reshape(next_state, [1, state_size])\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            agent.update_target_model()\n",
    "            print(\"episode: {}/{}, score: {}, e: {:.2}\"\n",
    "                  .format(e, episodes, reward, agent.epsilon))\n",
    "            break\n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.replay(batch_size)\n",
    "    if e % 10 == 0:\n",
    "        agent.save(\"./reinforcement-workshop.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "family_size = np.array(list(family_size_dict.values()))\n",
    "days_array = np.array(days)\n",
    "\n",
    "penalties_array = \n",
    "\n",
    "choice_dict_num = [{vv:i for i, vv in enumerate(di.values())} for di in choice_dict.values()]\n",
    "largest_choice_key = max(max(x.keys()) for x in choice_dict_num)\n",
    "choice_array_num = np.array([\n",
    "    [\n",
    "        choice[n] if n in choice else - 1 \n",
    "        for n in range(largest_choice_key+1)\n",
    "    ] \n",
    "    for choice in choice_dict_num\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = submission['assigned_day'].values\n",
    "start_score = cost_function(best, penalties_array, family_size, days_array)\n",
    "\n",
    "# loop over each family\n",
    "for fam_id in tqdm(range(len(best))):\n",
    "    # loop over each family choice\n",
    "    for pick in range(10):\n",
    "        day = choice_dict[fam_id][f'choice_{pick}']\n",
    "        temp = new.copy()\n",
    "        temp[fam_id] = day # add in the new pick\n",
    "        if cost_function(temp, penalties_array, family_size, days_array) < start_score:\n",
    "            new = temp.copy()\n",
    "            start_score = cost_function(new, penalties_array, family_size, days_array)\n",
    "\n",
    "submission['assigned_day'] = new\n",
    "score = cost_function(new, penalties_array, family_size, days_array)\n",
    "submission.to_csv(f'submission_{score}.csv')\n",
    "print(f'Score: {score}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
